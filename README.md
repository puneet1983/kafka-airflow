
##   Real-Time Trade ETL Pipeline

A Docker-based, real-time ETL pipeline to ingest, validate, process, and store trade events for analytics, compliance, and reporting. Built using **Apache Kafka**, **Apache Airflow**, **DuckDB**, and **Python**.

---

###       Overview

This project simulates a real-world trade processing platform where thousands of trades are generated, validated against business rules, and persisted in an analytical store. The pipeline ensures **data quality**, **version control**, and **auditability**.

---

## Architecture

**High-Level Flow**

```
Trade Producer(simulated data in json format) â†’ Kafka (raw_trades) â†’ Consumer & Validator â†’ DuckDB â†’ Airflow Orchestration â†’ Alerts
```

*(See `/docs/architecture.puml` for the full diagram.)*

---

##   Features

- Real-time ingestion using Apache Kafka  
- Schema validation & transformation  
- Business rule enforcement  
- Version-controlled trade processing  
- Orchestrated workflows with Airflow  
- Analytical storage using DuckDB  
- Docker-based local environment  
- Monitoring & alerting  

---

## Project Structure

```text
etl-trade-pipeline/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ DAG_1
â”‚   â””â”€â”€ DAG_2
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ consumer_batch.py    
â”‚   â””â”€â”€ load_delta.py
â”‚   â””â”€â”€ schema.py
â”‚   â””â”€â”€ trade_generator.py
â”‚   
â”œâ”€â”€ db/
â”‚   â””â”€â”€ trades.db
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ raw
â”‚         â””â”€â”€ batch_<timestamp>.json
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.puml
â””â”€â”€ README.md
```

---

## âš™ï¸ Prerequisites

- Docker Desktop  
- Docker Compose  
- Python 3.9+  
- Git  

---

## ğŸ› ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-org/etl-trade-pipeline.git
cd etl-trade-pipeline
```

---

### 2ï¸âƒ£ Start the Infrastructure
```bash
docker compose up -d
```

---

### 3ï¸âƒ£ Initialize Airflow

```bash
docker compose run airflow-webserver airflow db init

docker compose run airflow-webserver airflow users create   --username admin   --password <pwd>   --firstname Puneet   --lastname Jaiswal   --role Admin   --email <email>
```

---

### 4ï¸âƒ£ Initialize DuckDB

```bash
docker compose exec airflow-webserver python scripts/init_db.py
```

---

### 5ï¸âƒ£ Start Trade Producer

- Open Airflow UI â†’ http://localhost:8080  
- Enable DAG: `trade_simulation`  

---

### 6ï¸âƒ£ Run the Pipeline

- Open Airflow UI â†’ http://localhost:8080  
- Enable DAG: `trade_validation_pipeline`  

---

## ğŸ“Š Business Rules

| Rule | Action |
|------|--------|
| Incoming version < existing | Reject |
| Incoming version = existing | Replace |
| Maturity date < today | Reject |
| Maturity date passed | Mark as `EXPIRED` |

---

## ğŸ—„ï¸ Data Storage

**Database:** DuckDB (`db/trades.db`)

**Tables:**
- `valid_trades`  
- `invalid_trades`  
- `staging_valid_trades`
- `staging_rejected_trades`  

---

## ğŸ”” Monitoring & Alerts

- Airflow task retries & logs  
- Email alerts on task failure  

---

## ğŸ” Security & Governance

- Role-based access to DB  
- Encrypted Kafka traffic (in prod)  
- Audit logging enabled  

---

